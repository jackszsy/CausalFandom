{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Instructions for Step 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jointly determine cause and effect segments using causal feature learning. While Step1 determined which variables are candidate causes, we here aim to determine the range of values of those."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from func import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T08:27:27.265402Z",
     "start_time": "2023-08-23T08:27:27.261997Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Total cause list\n",
    "# Hide internal variables name\n",
    "cause_dict = {\n",
    "    'ticket':{\n",
    "        'pre1':[],\n",
    "        'treat':[],\n",
    "        'pre1treat':[]\n",
    "    },\n",
    "    'merch':{\n",
    "        'pre1':[],\n",
    "        'treat':[],\n",
    "        'pre1treat':[],\n",
    "    },\n",
    "    'share':{\n",
    "        'pre1':[],\n",
    "        'treat':[],\n",
    "        'pre1treat':[],\n",
    "    },\n",
    "    'stream':{\n",
    "        'pre1':[],\n",
    "        'treat':[],\n",
    "        'pre1treat':[],\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T08:27:28.954834Z",
     "start_time": "2023-08-23T08:27:28.942229Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "raw_input = load_data('../Data/CausalFandom_main_data.pickle')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T08:27:40.671573Z",
     "start_time": "2023-08-23T08:27:31.168681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Prepare data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "combine 8 potential causes data of 4 outcomes to put into XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Prepare raw data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# combine 4 outcomes and 8 potential causes\n",
    "stream_change = (raw_input[['streams_active_streams_following_four_weeks',\n",
    "  'streams_programmed_streams_following_four_weeks']].sum(axis = 1) - raw_input[['streams_active_streams_pre_period_1',\n",
    "  'streams_programmed_streams_pre_period_1']].sum(axis = 1)).rename('stream_change')\n",
    "\n",
    "share_change = (raw_input['shares_following_four_weeks'] - raw_input['shares_pre_period_1']).rename('share_change')\n",
    "\n",
    "ticket_change = (raw_input['tickets_following_four_weeks'] - raw_input['tickets_pre_period_1']).rename('ticket_change')\n",
    "\n",
    "merch_change = (raw_input['merch_pre_period_1'] - raw_input['merch_following_four_weeks']).rename('merch_change')\n",
    "\n",
    "outcome_colnames = ['share_change','ticket_change','stream_change','merch_change']\n",
    "\n",
    "allposscaues = []\n",
    "\n",
    "# Put 4 changes after all possible causes\n",
    "combinexy_data = pd.concat([raw_input[allposscaues],stream_change, share_change, ticket_change, merch_change],axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Filtering data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Filter both zero ot outcome in pre-treatment period and post-treatment period\n",
    "2. Filter extreme values\n",
    "2. Apply log sampling to all outcomes' data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove data that are both zero in pre and post\n",
    "idx_stream = (((raw_input['streams_active_streams_pre_period_1'] + raw_input['streams_programmed_streams_pre_period_1']) != 0 ) | ((raw_input['streams_active_streams_following_four_weeks'] + raw_input['streams_programmed_streams_following_four_weeks'] )!= 0 ))\n",
    "idx_share = ((raw_input['shares_pre_period_1']!= 0 ) | (raw_input['shares_following_four_weeks'] != 0 ))\n",
    "idx_merch = ((raw_input['merch_pre_period_1']!= 0 ) | (raw_input['merch_following_four_weeks'] != 0 ))\n",
    "idx_ticket = ((raw_input['tickets_pre_period_1']!= 0 ) | (raw_input['tickets_following_four_weeks'] != 0 ))\n",
    "\n",
    "streamdata = combinexy_data[idx_stream].reset_index(drop = True)\n",
    "sharedata = combinexy_data[idx_share].reset_index(drop = True)\n",
    "ticketdata = combinexy_data[idx_ticket].reset_index(drop = True)\n",
    "merchdata = combinexy_data[idx_merch].reset_index(drop = True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply llog sampling to all outcomes (further apply filtering on change of outcomes)\n",
    "streamsample = sample_data(streamdata[abs(streamdata['stream_change']) < 50].reset_index(drop = True),'log',50,keyword='stream_change')\n",
    "sharesample = sample_data(sharedata[abs(sharedata['share_change']) < 15].reset_index(drop = True),'log',200,keyword='share_change')\n",
    "ticketsample = sampledata(ticketdata[abs(ticketdata['ticket_change']) < 15].reset_index(drop = True),'log',200,keyword='ticket_change')\n",
    "merchsample = sampledata(merchdata[abs(merchdata['merch_change']) < 15].reset_index(drop = True),'log',200,keyword='merch_change')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine data of each outcome\n",
    "subset_combineoutcome = pd.concat([streamsample, sharesample, ticketsample, merchsample], axis = 0).reset_index(drop = True)\n",
    "\n",
    "# Filter data using bounds to exclude extreme values\n",
    "idxoutlier = (abs(subset_combineoutcome['merch_change'])<5) & (abs(subset_combineoutcome['ticket_change'])<50) & (abs(subset_combineoutcome['share_change'])<15) & (subset_combineoutcome['stream_change']<50) & (abs(subset_combineoutcome['streams_active_streams_treatment_period'])<100) & (subset_combineoutcome['shares_treatment_period'] <30) & (subset_combineoutcome['tickets_treatment_period']<20) & (subset_combineoutcome['streams_programmed_streams_treatment_period']<100) & (subset_combineoutcome['streams_active_streams_pre_period_1']<100) & (subset_combineoutcome['streams_programmed_streams_pre_period_1']<100) & (subset_combineoutcome['shares_pre_period_1']<30)\n",
    "subset1 = subset_combineoutcome[idxoutlier].reset_index(drop = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Perform scaling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# standardized scale\n",
    "scaled_data = subset1\n",
    "for item in outcome_colnames:\n",
    "    scaled_data[item] = (subset1[item]-subset1[item].mean(axis=0))/(subset1[item].std(axis = 0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualise the distribution of stream change after scaling\n",
    "plt.hist(scaled_data['stream_change'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Whether to use a sampled data to reduce running time\n",
    "# fracdata = scaled_data.sample(100000,random_state=727)\n",
    "fracdata = scaled_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Plot PCA on outcome and print linear combinations of PCs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the all outcomes in PCA space (before clustering)\n",
    "pca = PCA(n_components=4)\n",
    "pca_result = pca.fit_transform(fracdata[outcome_colnames])\n",
    "plt.scatter(pca_result[:,0],pca_result[:,1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the linear combination of PC1 and PC2\n",
    "pca_compos = pd.DataFrame(data = {\n",
    "             'PCA1':pca.components_[0],\n",
    "            'PCA2':pca.components_[1]\n",
    "}, index= outcome_colnames)\n",
    "print(pca_compos)\n",
    "print(pca.explained_variance_ratio_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Perform Causal Feature Learning (CFL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Regression of x on y (Using XGBoost)\n",
    "2. Cluster regression value f(x), to have x's causal class\n",
    "3. Represent all y using 'knn format'\n",
    "4. Cluster 'knn format' y to have y's causal class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Regression of x on y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(fracdata[allposscaues],fracdata[outcome_colnames], test_size=0.3, random_state=4242)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train XGBoost model and test on the test dataset\n",
    "model_lin = XGBRegressor()\n",
    "model_lin.fit(x_tr,y_tr)\n",
    "y_pred_lin = model_lin.predict(x_te)\n",
    "print('MAE',mean_absolute_error(y_te,y_pred_lin))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "inputx = fracdata[allposscaues]\n",
    "data_y = fracdata[outcome_colnames]\n",
    "numofcluster1 = 4\n",
    "\n",
    "# Regression\n",
    "fx = model_lin.predict(inputx)\n",
    "y_reg = fx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Cluster regression value f(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#1. cluster on the f(x)\n",
    "randomseed = int(time())%100*(int(time()*10)%10)\n",
    "kmeans = KMeans(n_clusters = numofcluster1, random_state=randomseed+7, n_init = 100).fit(y_reg)\n",
    "labelx = kmeans.labels_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Perform K nearest neighbors for each y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The kth neighbor to calculate the distance\n",
    "kthneighbor = 4\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=kthneighbor)\n",
    "n_dis_data = np.ones((len(data_y), 1))\n",
    "\n",
    "for i in tqdm(range(numofcluster1)):\n",
    "    data_y_clus = data_y[labelx == i]\n",
    "    y_clus = data_y_clus\n",
    "    neigh.fit(y_clus)\n",
    "    distance, idx_neigh = neigh.kneighbors(data_y, return_distance=True)\n",
    "    distance = distance[:, -1]\n",
    "    distance = distance.reshape(-1, 1)\n",
    "    n_dis_data = np.concatenate((n_dis_data, distance), axis = 1)\n",
    "\n",
    "n_dis_data_clean = n_dis_data[:, 1:]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Perform clustering on new representation of y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify number of clusters of y\n",
    "numofcluster2 = 4\n",
    "\n",
    "kmeans = KMeans(n_clusters = numofcluster2, random_state= randomseed+2, n_init = 10).fit(n_dis_data_clean)\n",
    "\n",
    "labely = kmeans.labels_.tolist()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Visualise clusters in PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Visualise clusters of y\n",
    "2. Visualise clusters of x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Visualise cluster of y in PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using the PCA result calculated before\n",
    "Yplot = pca_result\n",
    "\n",
    "colindex = 1\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.title('Cluster Plot')\n",
    "listran = list(set(labely))\n",
    "\n",
    "for type in range(len(listran)):\n",
    "    plt.scatter(Yplot[pd.Series(labely) == type][:,0], Yplot[pd.Series(labely) == type][:,1], label = f\"\"\"clus{type+1}\"\"\",marker = marker_list[type])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Perform PCA on x (before clustering)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "pca_result1 = pca.fit_transform(fracdata[allposscaues])\n",
    "plt.scatter(pca_result1[:,0],pca_result1[:,1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print linear combinations of PC1 and PC2\n",
    "pca_compos = pd.DataFrame(data = {\n",
    "             'PCA1':pca.components_[0],\n",
    "            'PCA2':pca.components_[1]\n",
    "}, index= allposscaues)\n",
    "print(pca_compos)\n",
    "print('Explained variance ratio',pca.explained_variance_ratio_)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Visualise cluster of x in PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using the PCA result calculated above\n",
    "Yplot = pca_result1\n",
    "\n",
    "colindex = 1\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.title('Cluster Plot')\n",
    "listran = list(set(labely))\n",
    "\n",
    "for type in range(len(listran)):\n",
    "    plt.scatter(Yplot[pd.Series(labelx) == type][:,0], Yplot[pd.Series(labelx) == type][:,1], label = f\"\"\"clus{type+1}\"\"\",marker = marker_list[type])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the confusion matrix of each x towards each y\n",
    "y_frac = confusion_matrix(labelx, labely)\n",
    "pd.DataFrame(y_frac)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Within cluster analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the coefficients of each outcome's causes within each cluster (detailed calculation see report Sec. 6.4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Calculate the coefficients of causes of each outcome within each cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine all the causes\n",
    "causelistall = cause_dict['stream']['treat'] + cause_dict['stream']['pre1']\n",
    "\n",
    "yname_list = ['stream','share','ticket','merch']\n",
    "\n",
    "# Prepare dictionary to store result\n",
    "resultdict = {}\n",
    "for yname in yname_list:\n",
    "    resultdict[yname] = {}\n",
    "    for yla in range(numofcluster2):\n",
    "        resultdict[yname][yla] = []\n",
    "\n",
    "# Prepare dictionary to store size\n",
    "sizedict = {}\n",
    "for yname in yname_list:\n",
    "    sizedict[yname] = {}\n",
    "    for yla in range(numofcluster2):\n",
    "        sizedict[yname][yla] = []\n",
    "\n",
    "# Calculate coefficient of each cause and store it for further analysis\n",
    "for yla in range(numofcluster2):\n",
    "    # Get subset of the data\n",
    "    xdatareg = inputx[pd.Series(labely) == yla][causelistall]\n",
    "    ydatareg = data_y[pd.Series(labely) == yla]\n",
    "    for yname in yname_list:\n",
    "        youtcomename = yname+'_change'\n",
    "        for xvar in cause_dict[yname]['treat']:\n",
    "            # Require the xvar>0 and extract this coef only\n",
    "            idx = xdatareg[xvar] > 0\n",
    "            variable_dataset_x = xdatareg[idx]\n",
    "            variable_dataset_y = ydatareg[idx]\n",
    "            if len(variable_dataset_y) == 0:\n",
    "                effect_var = 0\n",
    "            else:\n",
    "                # Using Linear regression to fit the data\n",
    "                model_lin_part = LinearRegression().fit(variable_dataset_x,variable_dataset_y[youtcomename])\n",
    "                # Extract the coefficient of corresponding causes from model\n",
    "                effect_var = model_lin_part.coef_[causelistall.index(xvar)]\n",
    "\n",
    "            resultdict[yname][yla].append(effect_var)\n",
    "            sizedict[yname][yla].append(len(variable_dataset_y))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Visualise coefficients and sizes of data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate outcomes to plot the heatmap of the coeffcients value\n",
    "listoftable = []\n",
    "for yname in yname_list:\n",
    "    currentdict = resultdict[yname]\n",
    "    datatable = pd.DataFrame(data = currentdict,index=cause_dict[yname]['treat'])\n",
    "    listoftable.append(datatable)\n",
    "\n",
    "    plt.figure(figsize=(datatable.shape[1] + 1,datatable.shape[0]))\n",
    "    sns.heatmap(datatable, annot= True, linewidth=.3)\n",
    "    plt.title(yname)\n",
    "    plt.xlabel('cluster number')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate outcomes to plot the heatmap of the size of data when calculating each coeffcient\n",
    "listoftablesize = []\n",
    "for yname in yname_list:\n",
    "    currentdict = sizedict[yname]\n",
    "    datatable = pd.DataFrame(data = currentdict,index=cause_dict[yname]['treat'])\n",
    "    listoftablesize.append(datatable)\n",
    "\n",
    "    plt.figure(figsize=(datatable.shape[1] + 1,datatable.shape[0]))\n",
    "    sns.heatmap(datatable, annot= True, linewidth=.3)\n",
    "    plt.title(yname)\n",
    "    plt.xlabel('cluster number')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Scaling all coefficients in terms of certain cause (inner variable hidden) to find the relationship between coeffcients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scaling all coefficients in terms of a cause [inner variable hidden]\n",
    "for yname in yname_list:\n",
    "    currentdict = resultdict[yname]\n",
    "    datatable = pd.DataFrame(data = currentdict,index=cause_dict[yname]['treat'])\n",
    "    multipletable = (datatable/datatable.loc['certain cause, inner variable hidden',:]).round(5)\n",
    "\n",
    "    plt.figure(figsize=(datatable.shape[1] + 1,datatable.shape[0]))\n",
    "    sns.heatmap(multipletable, annot= True, linewidth=.3)\n",
    "    plt.title(yname)\n",
    "    plt.xlabel('cluster number')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
