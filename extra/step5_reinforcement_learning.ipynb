{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Configuration:\n",
    "- Action: Set the 8 behaviors in treatment period as sequential actions, all users perform saves first, follows next ... until the 8th action streams programmed streams\n",
    "- Reward: The outcomes of the post period are the rewards towards each data, so there are four rewards in total.\n",
    "- Observation: The status after each action is an observation, so there are 8 observations for a data (user).\n",
    "- Timesteps: In this case, the actions are timesteps itself, timestep1 represents action 1, which is save, timestep2 represents action 2, which is follow ,etc.\n",
    "\n",
    "### Objectives:\n",
    "Find out the degree of reward that different actions can contributed to. e.g. can a save can lead to plus 2 of reward, can two follows can lead to minus 1 of reward, etc.\n",
    "Since the reward can only be known after all actions have been performed, a method that can let the reward redistributed among actions is key to the problem.\n",
    "\n",
    "### Data representation:\n",
    "- Action: An 8 dimensional vector, each position represents the corresponding action e.g. 1st position is save, 2nd position is follow.\n",
    "- Reward: A number, which is the outcome value\n",
    "- Observation: An 8 dimensional vector, each position also represents the corresponding action. Unlike action, observation records all the actions done before.\n",
    "\n",
    "## Methods used:\n",
    "- Rudder: Used to let the reward redistributed among timesteps (actions). The main task in Rudder is to train an LSTM model to predict the return of each sample at the end of the sequence. As auxiliary task, the model also be trained to predict the final return at every sequence position. This will allow us to use differences of predictions for contribution analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path,'rb') as file:\n",
    "        unpicker = pickle.Unpickler(file,encoding = 'latin1')\n",
    "        return unpicker.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Specify name of actions and rewards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = [\n",
    "    'saves_treatment_period',\n",
    "    'follows_treatment_period',\n",
    "    'playlists_treatment_period',\n",
    "    'tickets_treatment_period',\n",
    "    'merch_treatment_period',\n",
    "    'shares_treatment_period',\n",
    "    'streams_active_streams_treatment_period',\n",
    "    'streams_programmed_streams_treatment_period'\n",
    "]\n",
    "reward_list = [\n",
    "    'shares_following_four_weeks',\n",
    "    'merch_following_four_weeks',\n",
    "    'ticket_following_four_weeks',\n",
    "    'streams_active_streams_following_four_weeks',\n",
    "    'streams_programmed_streams_following_four_weeks'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Load data from disk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "clean_data1 = load_data('../Data/CausalFandom_main_data.pickle')\n",
    "\n",
    "clean_data2 = clean_data1.sample(frac=0.5, replace=False, random_state=41)\n",
    "\n",
    "clean_data3 = clean_data2.sample(frac=0.1, replace=False, random_state=41).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Implement dataset class used for pytorch training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, action_list, reward_aim):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.action_list = action_list\n",
    "        self.reward_aim = reward_aim\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        tmp = self.data.loc[[idx],:]\n",
    "        obs, act, rew = self.create_dataset(tmp, self.action_list, self.reward_aim)\n",
    "        return obs.astype(np.float32), act.astype(np.float32), rew.astype(np.float32)\n",
    "    # Transform the original data into 8d action, 8d observation and 1d reward\n",
    "    def create_dataset(self, data, action_list, reward_aim):\n",
    "        obs_act = data[action_list].to_numpy()\n",
    "        rewards = data[reward_aim].to_numpy()\n",
    "        num_data = obs_act.shape[0]\n",
    "        num_pos = obs_act.shape[1]\n",
    "        obs = np.zeros((num_data * num_pos, num_pos))\n",
    "        act = np.zeros((obs.shape[0], obs.shape[1]))\n",
    "        rew = np.zeros(obs.shape[0])\n",
    "        for i in range(num_data):\n",
    "            cur_row = obs_act[i,:]\n",
    "            for j in range(num_pos):\n",
    "                obs[i*num_pos+j : (i+1)*num_pos, j] = cur_row[j]\n",
    "                act[i*num_pos+j, j] = cur_row[j]\n",
    "            rew[(i+1)*num_pos-1] = rewards[i]\n",
    "        return obs, act, rew\n",
    "\n",
    "# Define 'streams_active_streams_following_four_weeks' as reward\n",
    "dataset = CustomDataset(clean_data3, action_list=action_list, reward_aim=reward_list[3])\n",
    "\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test type/size of obs,act,rew\n",
    "for data in dataloader:\n",
    "    obs, act, rew = data\n",
    "    print('obs:',obs.dtype)\n",
    "    print('act:',act.shape)\n",
    "    print('rew:',rew.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Custom LSTM training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 8\n",
    "act_dim = 8\n",
    "time_steps = act_dim = 8\n",
    "input_dim = obs_dim + act_dim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat((obs, act), dim=2)\n",
    "        out, _ = self.lstm(x)\n",
    "        # Using the prediction of last timestep (action) as output\n",
    "        # out = self.fc(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "lstm = LSTMModel(input_dim, hidden_dim, num_layers)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_obs, batch_act, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm(batch_obs, batch_act)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training finished.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Visualise two samples to see the redistribution performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2 samples\n",
    "obs0, a0, r0 = dataset[3]\n",
    "obs1, a1, r1 = dataset[70]\n",
    "\n",
    "# Apply reward redistribution model to the samples\n",
    "test_obs = torch.stack([torch.Tensor(obs0), torch.Tensor(obs1)], dim=0)\n",
    "test_act = torch.stack([torch.Tensor(a0), torch.Tensor(a1)], dim=0)\n",
    "test_rew = torch.stack([torch.Tensor(r0), torch.Tensor(r1)], dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = lstm(test_obs, test_act)\n",
    "\n",
    "print(predictions.shape)\n",
    "print('-------------')\n",
    "print(predictions[:, 1:])\n",
    "print(predictions[:, :-1])\n",
    "\n",
    "# Use the differences of predictions as redistributed reward\n",
    "redistributed_reward = predictions[:, 1:] - predictions[:, :-1]\n",
    "\n",
    "# For the first timestep we will take (0-predictions[:, :1]) as redistributed reward\n",
    "redistributed_reward = torch.cat([predictions[:, :1], redistributed_reward], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redistributed_reward = redistributed_reward.cpu().detach().numpy()\n",
    "rr0, rr1 = redistributed_reward[0], redistributed_reward[1]\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(8, 6), dpi=100)\n",
    "axes[0, 0].plot(obs0.argmax(-1) - 6)\n",
    "axes[0, 1].plot(obs1.argmax(-1) - 6)\n",
    "axes[0, 0].set_ylim(-6, 6)\n",
    "axes[0, 1].set_ylim(-6, 6)\n",
    "axes[0, 0].axhline(2, linestyle='--', color='r')\n",
    "axes[0, 1].axhline(2, linestyle='--', color='r')\n",
    "axes[0, 0].xaxis.grid(True)\n",
    "axes[0, 1].xaxis.grid(True)\n",
    "axes[0, 0].set_title('observations (sample 1)')\n",
    "axes[0, 1].set_title('observations (sample 2)')\n",
    "axes[0, 0].set_xlabel('Actions')\n",
    "axes[0, 1].set_xlabel('Actions')\n",
    "\n",
    "axes[1, 0].plot(a0.argmax(-1))\n",
    "axes[1, 1].plot(a1.argmax(-1))\n",
    "axes[1, 0].xaxis.grid(True)\n",
    "axes[1, 1].xaxis.grid(True)\n",
    "axes[1, 0].set_title('actions (sample 1)')\n",
    "axes[1, 1].set_title('actions (sample 2)')\n",
    "\n",
    "axes[1, 0].set_xlabel('Actions')\n",
    "axes[1, 1].set_xlabel('Actions')\n",
    "\n",
    "axes[2, 0].plot(r0)\n",
    "axes[2, 1].plot(r1)\n",
    "axes[2, 0].xaxis.grid(True)\n",
    "axes[2, 1].xaxis.grid(True)\n",
    "axes[2, 0].set_title('original rewards (sample 1)')\n",
    "axes[2, 1].set_title('original rewards (sample 2)')\n",
    "axes[2, 0].set_xlabel('Actions')\n",
    "axes[2, 1].set_xlabel('Actions')\n",
    "\n",
    "axes[3, 0].plot(rr0)\n",
    "axes[3, 1].plot(rr1)\n",
    "axes[3, 0].xaxis.grid(True)\n",
    "axes[3, 1].xaxis.grid(True)\n",
    "axes[3, 0].set_title('redistributed rewards (sample 1)')\n",
    "axes[3, 1].set_title('redistributed rewards (sample 2)')\n",
    "axes[3, 0].set_xlabel('Actions')\n",
    "axes[3, 1].set_xlabel('Actions')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Visualise the reward contribution that different actions have\n",
    "\n",
    "Using the reward difference between two timesteps as the reward of certain value of the action. e.g.\n",
    "- timestep1: reward=5, perform 5 times of action A\n",
    "- timestep2: reward=10, perform 9 times of action B ...\n",
    "The reward of action A equals to 5 is 4\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    obs, act, _ = dataset[idx]\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "    act_tensor = torch.tensor(act, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = lstm(obs_tensor.unsqueeze(0), act_tensor.unsqueeze(0)).squeeze().numpy()\n",
    "\n",
    "    all_predictions.append(prediction)\n",
    "\n",
    "# Transform tensor to numpy array then back to tensor again to reduce dimension\n",
    "all_predictions = torch.tensor(np.array(all_predictions)).unsqueeze(2)\n",
    "\n",
    "# Use the differences of predictions as redistributed reward\n",
    "redistributed_reward = all_predictions[:, 1:] - all_predictions[:, :-1]\n",
    "\n",
    "# For the first timestep we will take (0-predictions[:, :1]) as redistributed reward\n",
    "redistributed_reward = torch.cat([all_predictions[:, :1], redistributed_reward], dim=1)\n",
    "\n",
    "redistributed_reward = np.array(redistributed_reward.squeeze(2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2, figsize=(8, 6), dpi=100)\n",
    "\n",
    "axes[0,0].scatter(clean_data3[action_list[0]].to_numpy(), redistributed_reward[:,0], s=10)\n",
    "axes[0,0].set_xlabel('Value')\n",
    "axes[0,0].set_ylabel('Reward')\n",
    "axes[0,0].set_title(action_list[0])\n",
    "axes[0,1].scatter(clean_data3[action_list[1]].to_numpy(), redistributed_reward[:,1], s=10)\n",
    "axes[0,1].set_xlabel('Value')\n",
    "axes[0,1].set_ylabel('Reward')\n",
    "axes[0,1].set_title(action_list[1])\n",
    "axes[1,0].scatter(clean_data3[action_list[2]].to_numpy(), redistributed_reward[:,2], s=10)\n",
    "axes[1,0].set_xlabel('Value')\n",
    "axes[1,0].set_ylabel('Reward')\n",
    "axes[1,0].set_title(action_list[2])\n",
    "axes[1,1].scatter(clean_data3[action_list[3]].to_numpy(), redistributed_reward[:,3], s=10)\n",
    "axes[1,1].set_xlabel('Value')\n",
    "axes[1,1].set_ylabel('Reward')\n",
    "axes[1,1].set_title(action_list[3])\n",
    "axes[2,0].scatter(clean_data3[action_list[4]].to_numpy(), redistributed_reward[:,4], s=10)\n",
    "axes[2,0].set_xlabel('Value')\n",
    "axes[2,0].set_ylabel('Reward')\n",
    "axes[2,0].set_title(action_list[4])\n",
    "axes[2,1].scatter(clean_data3[action_list[5]].to_numpy(), redistributed_reward[:,5], s=10)\n",
    "axes[2,1].set_xlabel('Value')\n",
    "axes[2,1].set_ylabel('Reward')\n",
    "axes[2,1].set_title(action_list[5])\n",
    "axes[3,0].scatter(clean_data3[action_list[6]].to_numpy(), redistributed_reward[:,6], s=10)\n",
    "axes[3,0].set_xlabel('Value')\n",
    "axes[3,0].set_ylabel('Reward')\n",
    "axes[3,0].set_title(action_list[6])\n",
    "axes[3,1].scatter(clean_data3[action_list[7]].to_numpy(), redistributed_reward[:,7], s=10)\n",
    "axes[3,1].set_xlabel('Value')\n",
    "axes[3,1].set_ylabel('Reward')\n",
    "axes[3,1].set_title(action_list[7])\n",
    "\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen from the plots above, the reward towards each value of 8 behaviors,\n",
    "- Save can have a clear path\n",
    "- Follow's reward contribution range from -1 to 2\n",
    "- Playlist and share don't have clear pattern but their contribution almost lie below 0\n",
    "- In opposite, active stream and programmed stream's contributions both mainly larger than zero"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
